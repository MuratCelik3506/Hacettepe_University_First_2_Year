{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"reportcode.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ltzFblrBUq4Q"},"source":["<h1 align=\"center\">Hacettepe University<br>Department of Computer Science</h1> \n","<h2 align=\"center\">BBM 409 - Machine Learning Laboratory<br>2021 Fall</h2>\n","<h4 align=\"center\">Assignment 2<br>Due on November 19, 2021</h4>\n","\n"],"id":"ltzFblrBUq4Q"},{"cell_type":"markdown","metadata":{"id":"OaGq4B5BUq4X"},"source":["<table>\n","    <tr>\n","      <td><h4>Name Surname</h4></td>\n","      <td></td>\n","      <td><h4>Student ID</h4></td>\n","    </tr>\n","    <tr>\n","      <td><h4>Humeyra Uçar</h4></td>\n","      <td></td>\n","      <td><h4>21827957</h4></td>\n","    </tr>\n","    <tr>\n","      <td><h4>Murat Çelik</h4></td>\n","      <td></td>\n","      <td><h4>21827263</h4></td>\n","    </tr>\n","</table>\n"],"id":"OaGq4B5BUq4X"},{"cell_type":"markdown","metadata":{"id":"XsTFWiyIUq4Z"},"source":["## 1. Introduction </a>\n","\n","&emsp;&emsp; In this assignment, we implemented Decision Tree model by using ID3 algorithm on the Diabetes Risk Prediction dataset. A decision tree is a specific type of flow chart used to visualize the decision-making process by mapping out different courses of action.\n","In simple words, a decision tree is a structure that contains nodes (attributes) and branches and is built from a dataset. Each node is either used to make a decision (decision node) or represent an outcome (leaf node). <br>\n","&emsp;&emsp; We used the ID3 algorithm for create the tree. ID3 (Iterative Dichotomiser 3) is the algorithm that iteratively divides features into two or more groups at each step."],"id":"XsTFWiyIUq4Z"},{"cell_type":"markdown","metadata":{"id":"DO9C_Ng-Uq4a"},"source":["## 2. Background</a>\n","&emsp;&emsp;Before analyzing the model, we will conversation about which function we use to create the tree.\n","\n","### ID3 Algorithm </a>\n","&emsp;&emsp; The ID3 algorithm selects the best feature at each step while building a Decision Tree. It uses Information Gain to find the best feature. Information gain measures how well a given attribute separates the training examples according to their target classification. The feature with the highest Information Gain is selected as the best one. We calculated information using the following formula:<br>\n","<center> $\\normalsize Gain(S,A) = Entropy(S) - \\sum_{v\\epsilon\\{Values(A)\\}} (\\frac{|S_v|}{S})Entropy(S_v) $ </center>\n","<br>\n","&emsp;&emsp;In order to define information gain precisely, we used a measure entropy.  Entropy characterizes the (im)purity of an arbitrary collection of examples.\n","\n","&emsp;&emsp;You can see the entropy formula below:<br>\n","<center> $\\normalsize Entropy(S) = - p_+ log_2p_+  - p_- log_2p_- $ </center>\n","\n","\n","### Performance Metrics </a>\n","&emsp;&emsp;Performance metrics help determine how well the model generalizes on new data.  Different performance metrics are used to evaluate different Machine Learning algorithms. For example, we used Accuracy, Precision, Recall, F1 score.\n","\n","<b> Confusion Matrix</b><br>\n","&emsp;&emsp;A Confusion matrix is an NxN matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. \n","![image.png](attachment:image.png)\n","<b> Accuracy </b><br>  \n","&emsp;&emsp;Classification accuracy is the simplest metric to use and implement. It is calculated as you can see in the formula:\n","<center>  $ \\normalsize Accuracy = 100 * \\frac{num\\;of\\;correctly\\;classified\\;examples}{num\\;of\\;examples}$ </center>\n","\n","<b> Precision </b><br>\n","&emsp;&emsp;Precision evaluates how precise a model is in predicting positive labels. Precision is a\n","good measure to determine when the cost of false positives is high.\n","<center> $\\normalsize {Precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP} {+} \\mathrm{FP}}$ </center>\n","\n","<b> Recall </b><br>\n","&emsp;&emsp;Recall calculates the percentage of actual positives a model correctly identified. Recall-\n","When there is a high cost associated with false negatives.\n","<center> $ \\normalsize {Recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP} {+}\\mathrm{FN}}$ </center>\n","\n","<b> F1 </b><br>\n","&emsp;&emsp;This is the harmonic mean of Precision and Recall and gives a better measure of the\n","incorrectly classified cases than the Accuracy Metric.\n","<center>  $ \\normalsize F_{1}= 2 * \\frac{\\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$<center>\n","<br><br>    "],"id":"DO9C_Ng-Uq4a"},{"cell_type":"markdown","metadata":{"id":"ylFt8c7YUq4c"},"source":["<b> Comments about metrics: </b>\n","* Precision values show the accuracy of your positive predictions. If this value is high, it means that our error of telling people who are not sick that they are sick is low.\n","* Recall values show how much of the positive true values (sick people) we predicted correctly. The reason why this value is important compared to others is to say that you are not sick to a sick person, which can lead to fatal consequences. For this reason, our main goal should be to increase the accuracy of this value.\n","* Since the F1 score is calculated using precision and recall values, it is often used in problems where both values are important to us. For this data set, if we think that the wrong drugs given to people who are not sick will cause great harm (precision) and if we think that it will cause great harm if we miss the sick people (recall); We better use f1 score. "],"id":"ylFt8c7YUq4c"},{"cell_type":"markdown","metadata":{"id":"-S3ReiaxUq4d"},"source":["### Cross-validation\n","![resim.png](attachment:2598cf1d-4843-4b63-83ae-33661f9b03d8.png)\n","<br>\n","```\n","1. Shuffle data\n","2. Divide the data into k different folds.\n","3. At each stage, equate one fold to the test data and the remaining data to the train data\n","4. Export the train and test data to the model and try the model\n","5. Save accuracy values \n","\n","```"],"id":"-S3ReiaxUq4d"},{"cell_type":"code","metadata":{"id":"TJhSo7O_Uq4e","executionInfo":{"status":"ok","timestamp":1637327526342,"user_tz":-180,"elapsed":969,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["def cross_val_score(model, X, y, label, pruning, k_split=5):   \n","    idx = X.index.values.copy()\n","    np.random.seed(42)   \n","    np.random.shuffle(idx)\n","    folds_idx = np.split(idx, k_split) \n","    scores = pd.DataFrame()\n","    #scores = dict()\n","    score_accuracy = list()\n","    score_f1 = list()\n","    score_precision = list()\n","    score_recall = list()\n","    for i in range(k_split):\n","        tmp = folds_idx.copy()\n","        test_ind = tmp.pop(i)\n","        train_ind = [ _ for sub in tmp for _ in sub]\n","        X_train, X_test, y_train, y_test = X.iloc[train_ind], X.iloc[test_ind], y[train_ind], y[test_ind]\n","        model.fit(X_train, y_train, label,pruning)   # gives data to the model \n","        y_pred = model.predict(X_test)               # output for test data by the model \n","        score_f1.append(f1_score(y_test, y_pred, average=\"binary\", pos_label=\"Positive\"))\n","        score_precision.append(precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Positive\"))\n","        score_recall.append(recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Positive\"))\n","        score_accuracy.append(accuracy_score(y_test, y_pred))  \n","        \n","    scores[\"accuracy\"] = score_accuracy\n","    scores[\"precision\"] = score_precision\n","    scores[\"recall\"] = score_recall\n","    scores[\"f1\"] = score_f1\n","    scores = scores.T\n","    scores['min'] = scores.min(numeric_only=True, axis=1)\n","    scores['mean'] = scores.mean(numeric_only=True, axis=1)\n","    scores['max'] = scores.max(numeric_only=True, axis=1)\n","    return scores"],"id":"TJhSo7O_Uq4e","execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PCnx4pbFUq4h"},"source":["## 3. Required Libraries <a name='3'></a>"],"id":"PCnx4pbFUq4h"},{"cell_type":"code","metadata":{"id":"UBA3cyDmUq4i","executionInfo":{"status":"ok","timestamp":1637327526345,"user_tz":-180,"elapsed":13,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["import numpy as np \n","import pandas as pd\n","\n","    \n","import networkx as nx\n","from networkx.drawing.nx_pydot import graphviz_layout\n","\n","import matplotlib.pyplot as plt\n","import copy    \n","\n","from sklearn.model_selection import train_test_split \n","from sklearn.metrics import accuracy_score,f1_score,recall_score, precision_score"],"id":"UBA3cyDmUq4i","execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0QLRijJUq4j"},"source":["## 4. Decision Tree Algorithms</a>"],"id":"u0QLRijJUq4j"},{"cell_type":"markdown","metadata":{"id":"fP582ej4Uq4k"},"source":["&emsp;&emsp; We use tree object for create decision tree. Tree object has two class variable as attribute and branches. 'Attribute' is the node of tree. And 'branches' is a dictionary which is values are tree object and keys are branches. <br>\n","&emsp;&emsp; DecisionTreeClassifier class has methods like fit, predict, pruning etc. The <i>fit</i> method takes the data and creates a tree. It then assigns root of the tree to the root variable of the object. If pruning is required, a tree is created by splitting the data into two (train/val). The <i>predict</i> method is used for the model to make a decision. By looking at the attributes of the given data, it moves through the branches until it reaches the leaf node.\n","\n","```\n","   Tree creating algorithm:\n","1. For each attribute calculate information gain.\n","2. Select the attribute with the maximum information gain as the root node.\n","3. Split the dataset using the attribute that is root.\n","4. If all rows are in the same class(pure node), change the current root as a leaf node with the class.\n","5. Else repeat 1-4 step with rest of data and attributes until rows count equals 0 or there is no attribute to calculate information gain.\n","```\n"],"id":"fP582ej4Uq4k"},{"cell_type":"code","metadata":{"id":"hy0xaFgJUq4l","executionInfo":{"status":"ok","timestamp":1637327526671,"user_tz":-180,"elapsed":336,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["class DecisionTreeClassifier:\n","    \"\"\"\n","    data : the data which using train the model \n","    label : the class label \n","    root : the root node of the model(decision tree)\n","    prune_list : hold pruning attribute\n","    \"\"\"\n","    data = None \n","    label = None\n","    root = None\n","    prune_list = None\n","    initial_root = False\n","    # The method we generate and optimize our tree for our model \n","    def fit(self, X_train, y_train, label, pruning=False):    \n","        self.label = label\n","        self.root = Tree()\n","        self.prune_list = list()\n","        if pruning:\n","            # Train/Val/Test\n","            # We divide given train data by 2.(Train/Val)\n","            X_train_prune, X_dev, y_train_prune, y_dev = train_test_split(X_train, y_train, test_size=0.25, random_state=42) \n","            self.data = X_train_prune.copy()\n","            self.data[label] = y_train_prune.copy()\n","            Tree.create_tree(self.root, None, self.data, self.label, list(X_train.columns))\n","            Tree.stablize_tree(self.root, self.data, self.label)\n","            if self.initial_root:\n","                self.draw()\n","            self.pruning(self.root, self.data, self.label, X_dev, y_dev)      \n","        else:\n","            self.data = X_train.copy()\n","            self.data[label] = y_train.copy()\n","            Tree.create_tree(self.root, None, self.data, self.label, list(X_train.columns))\n","            Tree.stablize_tree(self.root, self.data, self.label)\n","            \n","    # The method which the model decides.\n","    def predict(self, X_test):\n","        y_pred = list()\n","        for x in range(len(X_test)):\n","            row = X_test.iloc[x]\n","            att = row[self.root.attribute]\n","            node = self.root.branches[att]\n","            while isinstance(node, Tree):\n","                att = row[node.attribute]\n","                node = node.branches[att]\n","            y_pred.append(node)\n","        return y_pred\n","    \n","    def draw(self):\n","        self.root.draw_tree()\n","    def change_initial_root(self):\n","        self.initial_root = True\n","    def pruning(self, node, data, label, X_dev, y_dev):\n","        prune_model = copy.deepcopy(self)        \n","        twig = dict()\n","        self.minGain(node, data, label, twig) # Catalog all twigs in the tree \n","        attribute = min(twig, key = twig.get) # Find the twig with the least Information Gain\n","        self.cut(prune_model.root, attribute, data, label) # Relabel twig as a leaf\n","       \n","        accuracy_prune = prune_model.check_accuracy(X_dev, y_dev)\n","        accuracy_prev =  self.check_accuracy(X_dev, y_dev) # Measure the accuracy value of your decision tree\n","        if accuracy_prune >= accuracy_prev:   # If ”Current Accuracy ≥ Last Accuracy” : Jump to ”pruning”\n","            self.prune_list.append(str(\"Prune attribute : \"+ attribute + \" , {Score acc_prune : \"+ str(accuracy_prune)+\", Score acc_prev : \" + str(accuracy_prev)+ \"\\n\"))\n","            self.root = prune_model.root\n","            self.pruning(self.root, data, label, X_dev, y_dev)\n","            \n","    def check_accuracy(self, X_dev, y_dev):\n","        y_pred = self.predict(X_dev)\n","        accuracy_prune = accuracy_score(y_pred, y_dev)\n","        return accuracy_prune\n","                \n","    def cut(self, node, attribute, data, label):\n","        for k, v in node.branches.items():\n","            if isinstance(v, str):\n","                continue\n","            if v.attribute == attribute:\n","                new_label= data[data[node.attribute] == k][label].value_counts().index[0]\n","                node.branches[k] = new_label\n","            else:\n","                next_node = v\n","                feature_value_data = data[data[node.attribute] == k]\n","                self.cut(next_node, attribute, feature_value_data, label)\n","                \n","    def minGain(self, root_node, data, label, twig):\n","        if False not in list(map(lambda x : isinstance(x, str), root_node.branches.values())):\n","            twig[root_node.attribute] = Tree.gain(data, root_node.attribute, label)\n","        else:\n","            for k, v in root_node.branches.items():\n","                next_node = v\n","                if not isinstance(v, str):\n","                    feature_value_data = data[data[root_node.attribute] == k]\n","                    self.minGain(next_node, feature_value_data, label, twig)\n","                    "],"id":"hy0xaFgJUq4l","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpxU7u6WUq4n","executionInfo":{"status":"ok","timestamp":1637327526672,"user_tz":-180,"elapsed":7,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["class Tree:\n","    \"\"\"\n","        attribute : name of Node (columns of data)\n","        branches : list of Edge (labels in columns)\n","    \"\"\"\n","    attribute = None\n","    branches = None\n","    \n","    def __init__(self, attribute=None):\n","        self.attribute = attribute\n","        self.branches = dict()\n","        \n","    def add(self, attribute, subtree):\n","        self.branches[attribute] = subtree\n","\n","    def __repr__(self):\n","        return '\"Node\" : \"' + str(self.attribute) + '\",\"Branches\":' + str(self.branches)+''\n","    \n","    # draw tree\n","    def print_tree(self):\n","        G = nx.DiGraph()\n","        G.add_node(self.attribute)\n","        for branch, node in list(self.branches.items()): \n","            if isinstance(node, Tree):\n","                G.add_edge(self.attribute, node.attribute, label=branch)\n","                H = node.print_tree()\n","                G = nx.compose(G, H)\n","            else:\n","                G.add_edge(self.attribute, node+str(\"_\"+self.attribute[:5]), label=branch)\n","        return G\n","    \n","    def draw_tree(self):\n","        fig = plt.figure(1, figsize=(20, 20), dpi=100)\n","        G = self.print_tree()\n","        pos = graphviz_layout(G, prog=\"dot\")\n","        edge_labels = nx.get_edge_attributes(G, 'label')\n","        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n","        nx.draw(G, pos, with_labels=True, font_size=15)\n","        plt.show()\n","    \n","    # Entropy and Gain Methods \n","    @staticmethod\n","    def max_gain(data, column, label):\n","        arr = dict()\n","        for x in column:\n","            arr[x] = Tree.gain(data, x, label)\n","        return max(arr, key=arr.get)\n","\n","    @staticmethod\n","    def entropyS(data, label):\n","        p = data[label].value_counts() / data.shape[0]\n","        return np.sum(-p * np.log2(p))\n","    \n","    @staticmethod\n","    def entropy(data, attribute, v, label):\n","        sv_ratio = data[data[attribute] == v][label].value_counts() / data[data[attribute] == v][label].shape[0]\n","        entropy_sv = np.sum(-sv_ratio * np.log2(sv_ratio))\n","        return entropy_sv\n","    \n","    @staticmethod\n","    def gain(data, attribute, label):\n","        gain = Tree.entropyS(data, label)    \n","        ratio_sv_s = data[attribute].value_counts() / data[attribute].shape[0]\n","        for v in data[attribute].unique():\n","            entropy_sv = Tree.entropy(data, attribute, v, label)\n","            gain -= ratio_sv_s[v] * entropy_sv\n","        return gain\n","    \n","    # Our main method creating the model\n","    @staticmethod\n","    def create_tree(root, prev_feature_value, train_data, label,columns):\n","        if train_data.shape[0] != 0 and len(columns)>0: \n","            \n","            subtree = \"invalid\"\n","            while subtree == \"invalid\":\n","                if len(columns)<=0:\n","                    return\n","                max_info_feature = Tree.max_gain(train_data, columns,label ) # we find the node to add to our tree \n","                columns.remove(max_info_feature)  # we delete the used node from our list \n","                # we create our subtree consisting of single node and edge \n","                subtree, train_data = Tree.generate_sub_tree(max_info_feature, train_data, label)\n","            next_root = None\n","\n","            if prev_feature_value != None:  # we add subtree to our tree \n","                new_node = subtree\n","                root.add(prev_feature_value, new_node)\n","                next_root = new_node\n","            else:   # we assign the node with the highest gain to our root node. \n","                root.attribute = subtree.attribute\n","                root.branches = subtree.branches\n","                next_root = subtree\n","\n","            for branch, node in list(next_root.branches.items()): # fill the edge of our tree \n","                if node == \"?\": \n","                    feature_value_data = train_data[train_data[max_info_feature] == branch] \n","                    Tree.create_tree(next_root, branch, feature_value_data, label,columns) \n"," \n","    @staticmethod\n","    def generate_sub_tree(feature_name, train_data, label):\n","        subtree = Tree(feature_name) # create a subtree with column with max gain \n","        feature_value_count_dict = dict(train_data[feature_name].value_counts())\n","        if len(feature_value_count_dict)< 2 :  # If our node has only one child remove it \n","            return \"invalid\",train_data\n","        for feature_value, count in feature_value_count_dict.items():\n","            feature_value_data = train_data[train_data[feature_name] == feature_value] \n","\n","            leaf_node_control = False\n","            for c in train_data[label].value_counts().keys(): #for each output class\n","                class_count = feature_value_data[feature_value_data[label] == c].shape[0] \n","\n","                # if node data is completely from a particular class then assign class to that node \n","                if class_count == count: \n","                    subtree.add(feature_value, c)     \n","                    train_data = train_data[train_data[feature_name] != feature_value] \n","                    leaf_node_control = True\n","            # if the node data is not entirely of a particular class, assign \"?\" to continue. \n","            if not leaf_node_control: \n","                    subtree.add(feature_value, \"?\")      \n","        return subtree, train_data \n","    \n","    @staticmethod\n","    def stablize_tree(root,data,label):\n","        # If data is empty, then below this new branch add a leaf node with label = most common value of data\n","        for k,v in root.branches.items():\n","            if isinstance(v, str):\n","                if v == \"?\":\n","                    root.branches[k] = data[label].value_counts().index[0]\n","                else :\n","                    continue\n","            else:\n","                Tree.stablize_tree(v, data[data[root.attribute] == k], label)\n","            "],"id":"xpxU7u6WUq4n","execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A70HpSy9Uq4o"},"source":["## 5. Part I: Diabetes Risk Prediction </a>"],"id":"A70HpSy9Uq4o"},{"cell_type":"markdown","metadata":{"id":"8rSo9K8GUq4p"},"source":["### 5.1 Data Analysis </a>\n","&emsp;&emsp; Diabetes dataset consists of 520 samples with 16 features two class types. Classes are \"Positive\" and \"Negative\". We have 16 attributes, including the continuous \"Age\" attribute. In order to use our features in training, we make discretization to the \"age\" feature.\n","<br>\n","&emsp;&emsp; Discretization transforms the continuous variables in our data into discrete data reduced to certain intervals. \n","\n","Attribute and Class Information:\n","1. Age (Continuous Attribute)\n","2. Gender (Discrete Attribute)\n","3. Polyuria (Discrete Attribute)\n","4. Polydipsia (Discrete Attribute)\n","5. Sudden Weight Loss (Discrete Attribute)\n","6. Weakness (Discrete Attribute)\n","7. Polyphagia (Discrete Attribute)\n","8. Genital Thrush (Discrete Attribute)\n","9. Visual Blurring (Discrete Attribute)\n","10. Itching (Discrete Attribute)\n","11. Irritability (Discrete Attribute)\n","12. Delayed Healing (Discrete Attribute)\n","13. Partial Paresis (Discrete Attribute)\n","14. Muscle Stiffness (Discrete Attribute)\n","15. Alopecia (Discrete Attribute)\n","16. Obesity (Discrete Attribute)\n","17. Class (Output Prediction Class Information, ”Positive” or ”Negative”)"],"id":"8rSo9K8GUq4p"},{"cell_type":"code","metadata":{"id":"LTbiTOboUq4p","executionInfo":{"status":"error","timestamp":1637327527440,"user_tz":-180,"elapsed":772,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}},"outputId":"f924c931-d180-4814-8a8c-d6d331789a6c","colab":{"base_uri":"https://localhost:8080/","height":432}},"source":["diabetes = pd.read_csv('diabetes_data_upload.csv')\n","diabetes['Age']=pd.cut(diabetes['Age'], 5, labels=['15-30', '31-45', '46-60', '61-75', '76-90']) #discretization\n","diabetes.head()"],"id":"LTbiTOboUq4p","execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-303378e04d49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiabetes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'diabetes_data_upload.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdiabetes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiabetes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'15-30'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'31-45'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'46-60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'61-75'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'76-90'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#discretization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdiabetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'diabetes_data_upload.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"jWZGKvgoUq4s"},"source":["### 5.2 Model Analysis <a name='5.2'></a>"],"id":"jWZGKvgoUq4s"},{"cell_type":"code","metadata":{"id":"-rIVg-U4Uq4t","executionInfo":{"status":"aborted","timestamp":1637327527428,"user_tz":-180,"elapsed":14,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["label = \"class\"\n","X = diabetes.drop([label], axis=1)\n","y = diabetes[label]"],"id":"-rIVg-U4Uq4t","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwkV_D6PUq4u","executionInfo":{"status":"aborted","timestamp":1637327527429,"user_tz":-180,"elapsed":14,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["model = DecisionTreeClassifier()\n","df_part1 = cross_val_score(model, X, y, label, False)\n","df_part1"],"id":"MwkV_D6PUq4u","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75Bsa_O2Uq4u"},"source":["&emsp;&emsp; Our lowest value in Precision value is 89%. It seems that we have achieved an precision of 100%. This indicates the low rate of predict \"sick\" to who are not sick. <br>\n","&emsp;&emsp; In our opinion, Recall value is the most important among these metrics. The efficiency of this value is important for us to be one step ahead of the disease. We must increase the efficiency of this value.<br>\n","&emsp;&emsp; We think that our model could be better with a minimum value of 80% in our F1 score, but it is still positive for the worst scenario. "],"id":"75Bsa_O2Uq4u"},{"cell_type":"markdown","metadata":{"id":"ay77TPdlUq4u"},"source":["## 6. Part II: Pruning Decision Tree</a>\n","#### Post-pruning\n","&emsp; &emsp; Pruninig is a technique applied on the tree so that decision trees can be run more efficiently and more accurately. Post-pruning, first build the decision tree and then remove any non-essential branches. You can see our algorithm below: \n","\n","```\n","   Pruning Algorithm:\n","1. Catalog all twigs in the tree\n","2. Find the twig with the least Information Gain\n","3. Remove all child nodes of the twig\n","4. Relabel twig as a leaf (Set the majority of ”Positive” or ”Negative”\n","as leaf value)\n","5. Measure the accuracy value of your decision tree model with removed\n","twig on the validation set (”Current Accuracy”)\n","6. If ”Current Accuracy ≥Last Accuracy” : Jump to ”Step1”\n","7. Else : Revert the last changes done in Step 3,4 and then terminate\n","```"],"id":"ay77TPdlUq4u"},{"cell_type":"code","metadata":{"id":"skQJ6FC3Uq4u","executionInfo":{"status":"aborted","timestamp":1637327527429,"user_tz":-180,"elapsed":14,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["model = DecisionTreeClassifier()\n","df_part2 = cross_val_score(model,X,y,\"class\",True)\n","df_part2"],"id":"skQJ6FC3Uq4u","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJZdIptWUq4v"},"source":["#### Pre-pruning\n","&emsp;&emsp; One of the ways to avoid overfitting is to check for cross-validation error at each stage of splitting the tree, if the error does not decrease significantly enough, we stop the tree-building process early before we produce leaves with very small samples. The disadvantage of pre-pruning is may may underfit by stopping too early.\n","\n"],"id":"EJZdIptWUq4v"},{"cell_type":"markdown","metadata":{"id":"m4DQKGF6Uq4v"},"source":["## 7. Error Analysis for Classification</a>"],"id":"m4DQKGF6Uq4v"},{"cell_type":"markdown","metadata":{"id":"_Wjp55NPUq4x"},"source":["&emsp;&emsp; We compare the error calculations of our pre-pruning model with our post-pruning model. "],"id":"_Wjp55NPUq4x"},{"cell_type":"code","metadata":{"id":"LmndyzUgUq4x","executionInfo":{"status":"aborted","timestamp":1637327527430,"user_tz":-180,"elapsed":15,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["df_part1[[\"min\",\"mean\",\"max\"]].join(df_part2[[\"min\",\"mean\",\"max\"]],lsuffix=\"_pre-prune\",rsuffix=\"_post-prune\")"],"id":"LmndyzUgUq4x","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1-w7tLDLUq4y"},"source":["&emsp;&emsp; When we look at the table, we see that our values increase with pruning. Pruning gives us both accuracy and complexity. "],"id":"1-w7tLDLUq4y"},{"cell_type":"code","metadata":{"id":"mccOVQLbUq4y","executionInfo":{"status":"aborted","timestamp":1637327527431,"user_tz":-180,"elapsed":15,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["df_part1[[\"mean\"]].join(df_part2[[\"mean\"]],lsuffix=\"_pre-prune\",rsuffix=\"_post-prune\")"],"id":"mccOVQLbUq4y","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GzRyTv0JUq4z"},"source":["#### Missclassified Examples\n","1. When we apply the discretization method to the age column, we make some transformations. In our data, only people with different age characteristics seem to be the same age with this technique. This creates a conflict in the data when the classes are different. This increases the error rate.\n","\n","2. When building our tree, at some leaf nodes we assign the most common value to that leaf because of the algorithm in data of that part. Most non-common data here increases the error.\n","\n","3. Although many rows in the training data has exactly same features, they are labeled differently. It will causes missclasify and it decrease accuracy."],"id":"GzRyTv0JUq4z"},{"cell_type":"code","metadata":{"id":"LSwIWqqgUq4z","executionInfo":{"status":"aborted","timestamp":1637327527432,"user_tz":-180,"elapsed":16,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["same_rows = [315, 321, 357, 89, 162, 267, 364, 455]\n","diabetes.iloc[same_rows]"],"id":"LSwIWqqgUq4z","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OckxkOtpUq4z"},"source":["## 8. Time </a>"],"id":"OckxkOtpUq4z"},{"cell_type":"code","metadata":{"id":"Oh4JIbBMUq41","executionInfo":{"status":"aborted","timestamp":1637327527434,"user_tz":-180,"elapsed":18,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["import time\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"id":"Oh4JIbBMUq41","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fd33WgtKUq41"},"source":["&emsp;&emsp;For our Decision tree model, we create our tree once with the fit method. This is the longest part of the algorithm. The Predict method operates on our tree that we created. For this reason, the duration of the fit method is greater than the duration of the predict method. "],"id":"Fd33WgtKUq41"},{"cell_type":"code","metadata":{"id":"-vky5gT0Uq42","executionInfo":{"status":"aborted","timestamp":1637327527436,"user_tz":-180,"elapsed":20,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["model = DecisionTreeClassifier()\n","start = time.time()\n","model.fit(X_train, y_train, \"class\",False)   \n","after_fit = time.time()\n","y_pred = model.predict(X_test)\n","after_predict = time.time()\n","f1_score(y_test,y_pred,average=\"binary\",pos_label=\"Positive\")\n","fit_time = after_fit-start\n","predict_time = after_predict - after_fit\n","print(\"Fitting Time : \", fit_time)\n","print(\"Predict Time : \", predict_time)"],"id":"-vky5gT0Uq42","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkxzckSuUq42"},"source":["&emsp;&emsp;The model we pruning is slower than the model we don't pruning because we do additional processing. We observe this with the following time. "],"id":"vkxzckSuUq42"},{"cell_type":"code","metadata":{"id":"39zxZV9xUq43","executionInfo":{"status":"aborted","timestamp":1637327527437,"user_tz":-180,"elapsed":20,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["model = DecisionTreeClassifier()\n","start = time.time()\n","model.fit(X_train, y_train, \"class\",True)  \n","after_fit = time.time()\n","y_pred = model.predict(X_test)\n","after_predict = time.time()\n","f1_score(y_test,y_pred,average=\"binary\",pos_label=\"Positive\")\n","fit_time = after_fit-start\n","predict_time = after_predict - after_fit\n","print(\"Fitting Time : \", fit_time)\n","print(\"Predict Time : \", predict_time)"],"id":"39zxZV9xUq43","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JiBKkO6PUq43"},"source":["## Optional"],"id":"JiBKkO6PUq43"},{"cell_type":"code","metadata":{"id":"TibETrPyUq43","executionInfo":{"status":"aborted","timestamp":1637327527438,"user_tz":-180,"elapsed":20,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["import pydot"],"id":"TibETrPyUq43","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUFhye10Uq44"},"source":["#### Pre-pruning Decision Tree"],"id":"kUFhye10Uq44"},{"cell_type":"code","metadata":{"id":"jJh1Gz2HUq44","executionInfo":{"status":"aborted","timestamp":1637327527439,"user_tz":-180,"elapsed":21,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["model = DecisionTreeClassifier()\n","model.change_initial_root()\n","model.fit(X_train, y_train, \"class\",True)   "],"id":"jJh1Gz2HUq44","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjtS3KPmUq44"},"source":["#### Post-pruning Decision Tree"],"id":"fjtS3KPmUq44"},{"cell_type":"code","metadata":{"id":"ZCATdjdDUq45","executionInfo":{"status":"aborted","timestamp":1637327527440,"user_tz":-180,"elapsed":22,"user":{"displayName":"Murat Çelik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9tiJtIORyAJKMoveFKb_Leeqz2CE_mMs5FqI-=s64","userId":"02057559685172804112"}}},"source":["for x in model.prune_list:\n","    print(x)\n","model.draw()"],"id":"ZCATdjdDUq45","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h6RLTSK6Uq46"},"source":["## References\n","- Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems by Aurélien Géron\n","- [towardsdatascience](https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)\n","- [Stanford CS229: Machine Learning](https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)"],"id":"h6RLTSK6Uq46"}]}